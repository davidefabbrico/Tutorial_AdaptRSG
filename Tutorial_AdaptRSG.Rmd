---
title: "Tutorial AdaptRSG"
author: "Davide Fabbrico"
date: "2024-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## How to Install the Package `AdaptiveAllocation`?

You can install the development version of AdaptiveAllocation from GitHub (<https://github.com/davidefabbrico/AdaptiveAllocation>) with:
```{r install, eval=FALSE}
# install.packages("devtools")
devtools::install_github("davidefabbrico/AdaptiveAllocation", force = T)
```

## How to Use the Package `AdaptiveAllocation`?

First, let's create some simulated data to test our algorithm. In this case, we use the dataset proposed in the article by Miller and Harrison to generate $n$ observations from a multivariate normal distribution of dimension $d$. Specifically, for a given dimensionality $d$, we draw $X_1, \ldots, X_n \sim N(m, I) + N(0, I) + N(-m, I)$, where $m = (3/\sqrt{d}, \ldots, 3/\sqrt{d}) \in \mathbb{R}^d$, and then normalize each dimension to have zero mean and unit variance.

```{r miller}
# number of dimension
d <- 2
# number of observation
n <- 1000
# mean miller and harrison
mu1 <- rep(3/sqrt(d), d)
mu2 <- rep(0, d)
mu3 <- rep(-3/sqrt(d), d)
# set the values for the precision
sigma1 <- diag(1, nrow = d)
sigma2 <- diag(1, nrow = d)
sigma3 <- diag(1, nrow = d)
# set the seed
set.seed(2)
# create the three cluster
cluster1 <- MASS::mvrnorm(floor(n/3), mu1, sigma1)
cluster2 <- MASS::mvrnorm(floor(n/3), mu2, sigma2)
cluster3 <- MASS::mvrnorm(floor(n/3), mu3, sigma3)
result <- rbind(cluster1, cluster2, cluster3)
# normalize the columns
for (i in 1:d) {
  result[,i] <- (result[,i] - mean(result[,i]))/sd(result[,i])
}
# add the true allocation
cluster_id <- rep(0:2, each = floor(n/3))

allData <- result
trueClusters <- cluster_id
```

Let’s visualize the generated data:

```{r plot_data, fig.width=10, fig.height=5, fig.align='center', echo=FALSE}
# load the package AdaptiveAllocation
library(AdaptiveAllocation)
scattPlot2d(data = allData)
```

## Adaptive Allocation MCMC

### Initial Settings

The MCMC algorithm requires some initial settings. In particular, we need to specify the number of iterations, the number of burn-in iterations, the thinning, the number of observations $m$ that we want to update at each iteration, and the number of clusters $K$.

```{r mcmc_settings}

iter <- 6000
burnin <- 0
thin <- 1
m <- 6
K <- 3
nout <- (iter-burnin)/thin

```

### Adaptive Settings

Our Adaptive MCMC algorithm requires some additional settings. First, we need to decide whether to set the `adaptive` option to `TRUE` or `FALSE`. If `adaptive = FALSE`, you can use any diversity function you prefer (refer to the Appendix for various implemented Diversity functions). However, if `adaptive = TRUE`, the diversity function will default to `diversity = Exponential` with an adaptive $\lambda$, which depends on the following expression: $\lambda_t = \lambda_0 \cdot \zeta^t + 1$, where $\lambda_0$ and $\zeta$ are parameters that can be specified within the `AdaptRSG` function. For now, let's demonstrate with the `adaptive = TRUE` option.

#### One replica

```{r adaptive_alg_one, fig.width=10, fig.height=5, fig.align='center'}

# load the package AdaptiveAllocation
library(AdaptiveAllocation)
# save the chain
myRes <- AdaptiveAllocation::AdaptRSG(allData, K = K, m = m, iteration = iter, iterTuning = iter,
                            burnin = burnin, thin = thin, gamma = 0.6,
                            method = "EB", hyper = c(1, 1, 0, 1, 1, 1), adaptive = T, updateProbAllocation = 5,
                      lambda0 = 30, zeta = 0.996, nSD = 0)
  # compute the ARIs
  myARI <- rep(0, nout)
  for (i in 1:nout) {
    myARI[i] <- mclust::adjustedRandIndex(myRes$Allocation[i,], trueClusters)
  }
  myTime <- myRes$Execution_Time
  saveMyChain <- myARI
  
  # plot ARIs
  iterPlot <- 1:nout
  plot(iterPlot, saveMyChain, type = "l", col = "#FF6961", xlab = "iteration", ylab = "ARI", ylim = c(-0.2, 1))

```


Let's note that the algorithm includes several parameters. Let's review them one by one. `iterTuning` represents the number of iterations for tuning the parameter $\lambda$. If `iterTuning = iter`, then $\lambda \rightarrow 1$; otherwise, $\lambda$ maintains the value at the `iterTuning` iteration for the remainder of the MCMC. $\gamma$ is the value in the following mixture $\alpha_t = \gamma \cdot \left(f(t) \cdot \alpha_{t-1} + g(t) \cdot D_t^{\lambda} \right) + (1-\gamma) \cdot U$, where $D$ is the diversity vector and $U$ is the uniform vector. The functions $f$ and $g$ are defined as $f(t) = \frac{t}{t+s}$ and $g(t) = \frac{s}{t+s}$. `method = EB` means that the hyperparameters of the priors are set using empirical Bayes. `hyper` includes various hyperparameters of the prior distributions. `updateProbAllocation` indicates the thinning number for updating the allocation matrix. `lambda0` and `zeta` are the parameters for updating $\lambda$. `nSD` is the number of standard deviations to add to the value $s$ (see the article for more details).

#### $n$ replicas

```{r adaptive_alg, fig.width=10, fig.height=5, fig.align='center'}

# number of replica
replica <- 5
# save the chain
saveMyChain <- matrix(0, ncol = replica, nrow = nout)
# Execution time
myTime <- matrix(0, nrow = nout, ncol = replica)
for (repl in 1:replica) {
    set.seed(repl)
    myRes <- AdaptiveAllocation::AdaptRSG(allData, K = K, m = m, iteration = iter, iterTuning = iter,
                            burnin = burnin, thin = thin, gamma = 0.6,
                            method = "EB", hyper = c(1, 1, 0, 1, 1, 1), adaptive = T, updateProbAllocation = 5,
                      lambda0 = 30, zeta = 0.996, nSD = 0)
    # compute the ARIs
    myARI <- rep(0, nout)
    for (i in 1:nout) {
      myARI[i] <- mclust::adjustedRandIndex(myRes$Allocation[i,], trueClusters)
    }
    myTime[,repl] <- myRes$Execution_Time
    saveMyChain[, repl] <- myARI
  }
  
  meanMyLoss <- apply(saveMyChain, 1, mean)
  # plot ARIs
  iterPlot <- 1:nout
  par(mfrow = c(1, 2))
  plot(iterPlot, meanMyLoss, type = "l", col = "#FF6961", xlab = "iteration", ylab = "ARI", ylim = c(-0.2, 1))
  # my adaptive
  lbMy <- apply(saveMyChain, 1, quantile, 0.05)
  ubMy <- apply(saveMyChain, 1, quantile, 0.95)
  polygon(x = c(iterPlot, rev(iterPlot)), y = c(lbMy, rev(ubMy)), col = rgb(1, 0, 0, alpha = 0.2), border = NA)
  myTime <- myTime/1000000
  boxplot(as.vector(myTime), main = "", ylab = "Execution Time (sec.)", col = "lightpink")

```

### Compare the `AdaptRSG` with the `SSG` and the `RSG`

```{r comparison_alg, fig.width=10, fig.height=5, fig.align='center'}

# save the chain
saveRandomChain <- saveMyChain <- saveSysChain <- matrix(0, ncol = replica, nrow = nout)
myTime <- randomTime <- sysTime <- matrix(0, nrow = nout, ncol = replica)
  for (repl in 1:replica) {
    set.seed(repl)
    myRes <- AdaptiveAllocation::AdaptRSG(allData, K = K, m = m, iteration = iter, iterTuning = iter,
                            burnin = burnin, thin = thin, gamma = 0.6,
                            method = "EB", hyper = c(1, 1, 0, 1, 1, 1), adaptive = T, updateProbAllocation = 5,
                      lambda0 = 30, zeta = 0.996, nSD = 0)
    SysRes <- AdaptiveAllocation::ssg(allData, K = K, iteration = iter, method = "EB", hyper = c(1, 1, 0, 1, 1, 1), burnin = burnin, thin = thin, trueAll = T)
    RandomRes <- AdaptiveAllocation::rssg(allData, K = K, m = m, iteration = iter,
                      burnin = burnin, thin = thin, method = "EB",
                      hyper = c(1, 1, 0, 1, 1, 1))
    # compute the ARIs
    myARI <- rep(0, nout)
    SysARI <- rep(0, nout)
    RandomARI <- rep(0, nout)
    for (i in 1:nout) {
      myARI[i] <- mclust::adjustedRandIndex(myRes$Allocation[i,], trueClusters)
      SysARI[i] <- mclust::adjustedRandIndex(SysRes$Allocation[i,], trueClusters)
      RandomARI[i] <- mclust::adjustedRandIndex(RandomRes$Allocation[i,], trueClusters)
    }
    saveMyChain[, repl] <- myARI
    saveSysChain[, repl] <- SysARI
    saveRandomChain[, repl] <- RandomARI
    
    myTime[,repl] <- myRes$Execution_Time
    randomTime[,repl] <- RandomRes$Execution_Time
    sysTime[,repl] <- SysRes$Execution_Time
  }
  
  meanMyLoss <- apply(saveMyChain, 1, mean)
  meanSysLoss <- apply(saveSysChain, 1, mean)
  meanRandomLoss <- apply(saveRandomChain, 1, mean)
  meanMSysLoss <- mean(meanSysLoss[-c(1:(iter/2))])
  # plot ARIs
  iterPlot <- 1:nout
  plot(iterPlot, meanMyLoss, type = "l", col = "#FF6961", xlab = "iteration", ylab = "ARI", ylim = c(-0.2, 1))
  abline(h = meanMSysLoss, col = "#AEC6CF", lwd = 2)
  lines(iterPlot, meanRandomLoss, col = "#77DD77")
  lines(iterPlot, meanSysLoss, col = "lightblue")
  x_vert <- ceiling((n/m)*(K-1)) + ceiling(sqrt((n/m)*(K-1)*K))
  abline(v = x_vert, col = "blue", lty = 2)
  # systematic
  lbSys <- apply(saveSysChain, 1, quantile, 0.05)
  ubSys <- apply(saveSysChain, 1, quantile, 0.95)
  polygon(x = c(iterPlot, rev(iterPlot)), y = c(lbSys, rev(ubSys)), col = rgb(0, 0, 1, alpha = 0.2), border = NA)
  # random band
  lbRandom <- apply(saveRandomChain, 1, quantile, 0.05)
  ubRandom <- apply(saveRandomChain, 1, quantile, 0.95)
  polygon(x = c(iterPlot, rev(iterPlot)), y = c(lbRandom, rev(ubRandom)), col = rgb(0, 1, 0, alpha = 0.2), border = NA)
  # my adaptive
  lbMy <- apply(saveMyChain, 1, quantile, 0.05)
  ubMy <- apply(saveMyChain, 1, quantile, 0.95)
  polygon(x = c(iterPlot, rev(iterPlot)), y = c(lbMy, rev(ubMy)), col = rgb(1, 0, 0, alpha = 0.2), border = NA)
  legend("bottomright", legend = c("SSG", "RSG", "AdaptRSG"), col = c("#AEC6CF", "#77DD77", "#FF6961"), lty = 1, cex = 0.8, lwd = 2.5)
  
  nIterMyLoss <- rep(iter, replica)
  nIterRandLoss <- rep(iter, replica)
  nIterSysLoss <- rep(iter, replica)
  for (repl in 1:replica) {
    for (i in 1:nout) {
      if (saveMyChain[i, repl] <= meanMSysLoss + 0.01 && saveMyChain[i, repl] >= meanMSysLoss - 0.01) {
        nIterMyLoss[repl] <- i
        break
      } else {
        nIterMyLoss[repl] <- iter
      }
    }
  }
  for (repl in 1:replica) {
    for (i in 1:nout) {
      if (saveRandomChain[i, repl] <= meanMSysLoss + 0.01 && saveRandomChain[i, repl] >= meanMSysLoss - 0.01) {
        nIterRandLoss[repl] <- i
        break
      } else {
        nIterRandLoss[repl] <- iter
      }
    }
  }
  for (repl in 1:replica) {
    for (i in 1:nout) {
      if (saveSysChain[i, repl] <= meanMSysLoss + 0.01 && saveSysChain[i, repl] >= meanMSysLoss - 0.01) {
        nIterSysLoss[repl] <- i
        break
      } else {
        nIterSysLoss[repl] <- iter
      }
    }
  }
  MyTimeRepl <- RandomTimeRepl <- SysTimeRepl <- c()
  for (repl in 1:replica) {
    SysTimeRepl[repl] <- sysTime[nIterMyLoss[repl], repl]
    MyTimeRepl[repl] <- myTime[nIterMyLoss[repl], repl]
    RandomTimeRepl[repl] <- randomTime[nIterRandLoss[repl], repl]
  }
  MyTimeRepl <- MyTimeRepl/1000000
  RandomTimeRepl <- RandomTimeRepl/1000000
  SysTimeRepl <- SysTimeRepl/1000000
  boxplot(SysTimeRepl, MyTimeRepl, RandomTimeRepl, 
          names = c("SSG", "AdaptRSG", "RSG"), 
          main = "",
          ylab = "Execution Time to Convergence (sec.)",
          col = c("lightblue", "lightpink", "lightgreen"))

```

Let's visualize the clusters structure obtained from the final chain of `AdaptRSG` and `RSG`.

```{r}
# install.packages("ellipse")
library(ellipse)
```

```{r cluster_result, fig.width=10, fig.height=5, fig.align='center', echo = FALSE}
par(mfrow = c(1, 2))
plot(allData[,1], allData[,2], xlab = "V1", ylab = "V2", main = "RSG")
    centroide1 <- RandomRes$Mu[[nout]][1,]
    covarianza1 <- RandomRes$Precision[[nout]][1,]
    covarianza1 <- diag(1/covarianza1)
    centroide2 <- RandomRes$Mu[[nout]][2,]
    covarianza2 <- RandomRes$Precision[[nout]][2,]
    covarianza2 <- diag(1/covarianza2)
    centroide3 <- RandomRes$Mu[[nout]][3,]
    covarianza3 <- RandomRes$Precision[[nout]][3,]
    covarianza3 <- diag(1/covarianza3)
    ellisse1 <- ellipse(covarianza1, centre=centroide1, level=0.95)
    lines(ellisse1, col="#FF6961", lwd = 3) 
    ellisse2 <- ellipse(covarianza2, centre=centroide2, level=0.95)
    lines(ellisse2, col="#0096FF", lwd = 3) 
    ellisse3 <- ellipse(covarianza3, centre=centroide3, level=0.95)
    lines(ellisse3, col="#77DD77", lwd = 3)  
    # ys.sleep(0.1)
    
    plot(allData[,1], allData[,2], xlab = "V1", ylab = "V2", main = "AdaptRSG")
    centroide1 <- myRes$Mu[[nout]][1,]
    covarianza1 <- myRes$Precision[[nout]][1,]
    covarianza1 <- diag(1/covarianza1)
    centroide2 <- myRes$Mu[[nout]][2,]
    covarianza2 <- myRes$Precision[[nout]][2,]
    covarianza2 <- diag(1/covarianza2)
    centroide3 <- myRes$Mu[[nout]][3,]
    covarianza3 <- myRes$Precision[[nout]][3,]
    covarianza3 <- diag(1/covarianza3)
    ellisse1 <- ellipse(covarianza1, centre=centroide1, level=0.95)
    lines(ellisse1, col="#FF6961", lwd = 3) 
    ellisse2 <- ellipse(covarianza2, centre=centroide2, level=0.95)
    lines(ellisse2, col="#0096FF", lwd = 3) 
    ellisse3 <- ellipse(covarianza3, centre=centroide3, level=0.95)
    lines(ellisse3, col="#77DD77", lwd = 3) 
```

### Diversity

Let’s visualize the points with the highest diversity at the end of the algorithm.
```{r diversity, fig.width=10, fig.height=5, fig.align='center'}
iter <- 10000
myRes <- AdaptiveAllocation::AdaptRSG(allData, K = K, m = m, iteration = iter, iterTuning = iter,
                            burnin = burnin, thin = thin, gamma = 0.6,
                            method = "EB", hyper = c(1, 1, 0, 1, 1, 1), adaptive = T, updateProbAllocation = 5,
                      lambda0 = 30, zeta = 0.996, nSD = 0)
AdaptiveAllocation::scattPlot2d(res = myRes, data = allData, diversity = T)
```
